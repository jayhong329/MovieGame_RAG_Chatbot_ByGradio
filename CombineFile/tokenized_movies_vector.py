import pandas as pd
import faiss
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer

# å…¨å±€è¨­ç½®
MODEL_NAME = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
EXCEL_FILE = 'tokenized_descriptions.xlsx'
VECTOR_INDEX_PATH = 'tokenized_movies_vector.index'
IDS_PATH = 'tokenized_movies_ids.pkl'

# è®€å–æ–·è©çµæœ Excel
def read_tokenized_excel(file_path):
    """è®€å–å·²ç¶“äººå·¥æª¢æŸ¥çš„æ–·è©çµæœ"""
    print(f"ğŸ“¥ å¾ {file_path} è®€å–å·²æ–·è©è³‡æ–™...")
    df = pd.read_excel(file_path)

    if 'movie_title' not in df.columns or 'tokenized_corpus' not in df.columns:
        raise ValueError("âŒ Excel æª”æ¡ˆå¿…é ˆåŒ…å« 'movie_title' èˆ‡ 'tokenized_corpus' æ¬„ä½ï¼")

    df.dropna(subset=['tokenized_corpus'], inplace=True)
    df['id'] = range(1, len(df) + 1)  # è‡ªå‹•ç”Ÿæˆå”¯ä¸€ ID
    return df

# èªç¾©å‘é‡ç”Ÿæˆ
def generate_embeddings(texts, model_name):
    """ä½¿ç”¨ Sentence-BERT ç”Ÿæˆèªç¾©å‘é‡"""
    model = SentenceTransformer(model_name)
    embeddings = model.encode(
        texts,
        batch_size=16,
        show_progress_bar=True
    )
    # å‘é‡æ­£è¦åŒ– (ä½¿ç”¨ Inner Product ç­‰åŒæ–¼ Cosine Similarity)
    embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
    return embeddings

# å‰µå»º FAISS å‘é‡ç´¢å¼•ä¸¦ä¿å­˜
def create_faiss_index(embeddings, ids, index_path):
    """ä½¿ç”¨ FAISS å»ºç«‹å‘é‡ç´¢å¼•ä¸¦ä¿å­˜"""
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatIP(dimension)  # ä½¿ç”¨ Inner Product
    index = faiss.IndexIDMap(index)
    index.add_with_ids(embeddings.astype('float32'), ids)

    faiss.write_index(index, index_path)
    print(f"ğŸ’¾ å‘é‡ç´¢å¼•å·²ä¿å­˜è‡³ï¼š{index_path}")

# ä¿å­˜ ID å°æ˜ è¡¨
def save_ids_mapping(df, ids_path):
    """å°‡ ID å°æ˜ è¡¨ä¿å­˜ç‚º pkl"""
    ids_dict = {
        row['id']: {
            'title': row['movie_title'],
            'tokens': row['tokenized_corpus']
        }
        for _, row in df.iterrows()
    }
    with open(ids_path, 'wb') as f:
        pickle.dump(ids_dict, f)
    print(f"ğŸ’¾ ID å°æ˜ è¡¨å·²ä¿å­˜è‡³ï¼š{ids_path}")

# ä¸»ç¨‹åº
def main():
    try:
        # è®€å–å·²æ–·è©çš„æè¿°
        df = read_tokenized_excel(EXCEL_FILE)

        # èªç¾©å‘é‡ç”Ÿæˆ
        print("ğŸš€ æ­£åœ¨ç”Ÿæˆèªç¾©å‘é‡...")
        embeddings = generate_embeddings(
            texts=df['tokenized_corpus'].tolist(),
            model_name=MODEL_NAME
        )

        # å‰µå»º FAISS å‘é‡ç´¢å¼•
        create_faiss_index(
            embeddings=embeddings,
            ids=df['id'].values.astype('int64'),
            index_path=VECTOR_INDEX_PATH
        )

        # ä¿å­˜ ID å°æ˜ 
        save_ids_mapping(df, IDS_PATH)

        print("ğŸ‰ èªç¾©å‘é‡ç´¢å¼•å»ºç«‹å®Œæˆï¼")

    except Exception as e:
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

if __name__ == "__main__":
    main()
